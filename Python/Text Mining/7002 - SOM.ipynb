{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54497c3-100a-42ae-b31b-4c1e6622dd11",
   "metadata": {},
   "source": [
    "## __Text mining y Procesamiento de Lenguaje Natural (NLP)__\n",
    "\n",
    "__Profesor__: Anthony D. Cho\n",
    "\n",
    "__Tema__: Clustering de documentos\n",
    "\n",
    "__Método__: K-Means\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2a9f9-3531-473c-87e7-a187b2062341",
   "metadata": {},
   "source": [
    "__Dependencias__\n",
    "\n",
    "```{python}\n",
    "    python -m pip install nltk spacy simpsom\n",
    "    python -m spacy download en_core_web_sm\n",
    "    python -m spacy download es_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247e86e-b15c-45a0-a7c4-be1be7e77271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d30880a",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dff896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "from string import punctuation\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy import load\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from simpsom import SOMNet\n",
    "\n",
    "## Instancia del modelo de lenguaje\n",
    "nlp = load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d89bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca90efb",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encontrar la ruta de cada archivo de interes\n",
    "path_docs = glob('*/doc*.txt')\n",
    "\n",
    "## Almacenamiendo de contenido de los documentos e id (nombre del archivo)\n",
    "corpus, doc_id = [], [] \n",
    "\n",
    "## Incio de proceso de carga de documentos\n",
    "if len(path_docs):\n",
    "    for file in path_docs:\n",
    "\n",
    "        ## Se carga el texto\n",
    "        text = open(file, 'r', encoding='utf-8').read()\n",
    "        \n",
    "        ## Se almacena el texto\n",
    "        corpus.append(text)\n",
    "        \n",
    "        id = file.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "        ## Se almacena el id\n",
    "        doc_id.append(id)\n",
    "else:\n",
    "    print('No corpus have found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a229a6",
   "metadata": {},
   "source": [
    "#### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de textos\n",
    "cleanTexts = []\n",
    "\n",
    "for doc in corpus:\n",
    "\n",
    "    # ## Remover numeros y puntuaciones\n",
    "    doc = re.sub(r'[\\\"\\¿\\°\\d+]', '', doc)\n",
    "    doc = [s for s in doc if s not in punctuation]\n",
    "    doc = ''.join(doc)\n",
    "\n",
    "    ## Normalización y remover stopwords\n",
    "    documento = nlp(doc.lower())\n",
    "    tokens = [word.text for word in documento]\n",
    "    doc = [word for word in tokens if word not in STOP_WORDS]\n",
    "    doc = ' '.join(doc)\n",
    "    doc = re.sub(pattern='\\s+', repl=' ', string=doc)\n",
    "    \n",
    "    ## Aplicar lemmatización\n",
    "    documento = nlp(doc)\n",
    "    lemmas = [word.lemma_ for word in documento]\n",
    "    doc = ' '.join(lemmas)\n",
    "    doc = re.sub(pattern='\\s+', repl=' ', string=doc)\n",
    "\n",
    "    ## Almacenado de contenido procesado\n",
    "    cleanTexts.append(doc)\n",
    "\n",
    "## Mostar contenido procesado\n",
    "cleanTexts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instancia del modelo\n",
    "model = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "## Ajuste del modelo y retorno de TF matrix como (docs, term)\n",
    "tf_sparse = model.fit_transform(cleanTexts)\n",
    "\n",
    "## Extraer la lista de los palabras que representan las columnas de la matriz generada\n",
    "vocabulary = model.get_feature_names_out()\n",
    "\n",
    "print('(shape) TFxIDF (terms, docs): {}'.format(tf_sparse.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0f898d7",
   "metadata": {},
   "source": [
    "#### SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a341ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instancia del modelo\n",
    "model = SOMNet(net_height=10, net_width=10, \n",
    "               data=tf_sparse.toarray(), \n",
    "               random_seed=20221022)\n",
    "\n",
    "## Ajuste del modelo\n",
    "model.train(start_learning_rate=0.01, epochs=1000)\n",
    "\n",
    "## Guardar el modelo ajustado\n",
    "#model.save('SOM_simpsom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights difference on SOM grid mapping display\n",
    "model.diff_graph(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Asignar las etiquetas mediante el modelo ajustado\n",
    "clusters = model.cluster(array=tf_sparse.toarray(), show=True)\n",
    "print('Cantidad de clusteres:', len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdb3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

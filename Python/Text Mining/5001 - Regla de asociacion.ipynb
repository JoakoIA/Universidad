{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54497c3-100a-42ae-b31b-4c1e6622dd11",
   "metadata": {},
   "source": [
    "## __Text mining y Procesamiento de Lenguaje Natural (NLP)__\n",
    "\n",
    "__Profesor__: Anthony D. Cho\n",
    "\n",
    "__Tema__: Regla de asociación\n",
    "\n",
    "__Método__: APRIORI\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2a9f9-3531-473c-87e7-a187b2062341",
   "metadata": {},
   "source": [
    "__Dependencias__\n",
    "\n",
    "```{python}\n",
    "    python -m pip install nltk spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "    python -m spacy download es_core_news_sm\n",
    "    \n",
    "    python -m pip install mlxtend\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247e86e-b15c-45a0-a7c4-be1be7e77271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d30880a",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3772bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "from pandas import DataFrame\n",
    "\n",
    "from string import punctuation\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy import load\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "## Instancia del modelo de lenguaje\n",
    "nlp = load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d89bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca90efb",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encontrar la ruta de cada archivo de interes\n",
    "path_docs = glob('*/doc*.txt')\n",
    "\n",
    "## Almacenamiendo de contenido de los documentos e id (nombre del archivo)\n",
    "corpus, doc_id = [], [] \n",
    "\n",
    "## Incio de proceso de carga de documentos\n",
    "if len(path_docs):\n",
    "    for file in path_docs:\n",
    "\n",
    "        ## Se carga el texto\n",
    "        text = open(file, 'r', encoding='utf-8').read()\n",
    "        \n",
    "        ## Se almacena el texto\n",
    "        corpus.append(text)\n",
    "        \n",
    "        id = file.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "        ## Se almacena el id\n",
    "        doc_id.append(id)\n",
    "else:\n",
    "    print('No corpus have found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f1469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a229a6",
   "metadata": {},
   "source": [
    "#### Preprocesamiento\n",
    "\n",
    "Extracción de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de textos\n",
    "cleanTexts = []\n",
    "\n",
    "## Pattern extraction\n",
    "pattern = '(\\w+)/(PROPN|NOUN)'\n",
    "\n",
    "for doc in corpus:\n",
    "\n",
    "    ## Extraer las entidades\n",
    "    documento = nlp(doc)\n",
    "    text_POS = ''.join( f'{word.text}/{word.pos_} ' for word in documento )\n",
    "    text_POS = text_POS.rstrip()\n",
    "    word_list = [w for (w,t) in re.findall(pattern=pattern, string=text_POS)]\n",
    "    doc = ' '.join(word_list)\n",
    "    \n",
    "    # ## Remover numeros y puntuaciones\n",
    "    doc = re.sub(r'[\\\"\\¿\\°\\d+]', '', doc)\n",
    "    doc = [s for s in doc if s not in punctuation]\n",
    "    doc = ''.join(doc)\n",
    "\n",
    "    ## Normalización y remover stopwords\n",
    "    documento = nlp(doc.lower())\n",
    "    tokens = [word.text for word in documento]\n",
    "    doc = [word for word in tokens if word not in STOP_WORDS]\n",
    "    doc = ' '.join(doc)\n",
    "    doc = re.sub(pattern='\\s+', repl=' ', string=doc)\n",
    "    \n",
    "    ## Aplicar lemmatización\n",
    "    documento = nlp(doc)\n",
    "    lemmas = [word.lemma_ for word in documento]\n",
    "    doc = ' '.join(lemmas)\n",
    "    doc = re.sub(pattern='\\s+', repl=' ', string=doc)\n",
    "\n",
    "    ## Almacenado de contenido procesado\n",
    "    cleanTexts.append(doc)\n",
    "\n",
    "## Mostar contenido procesado\n",
    "cleanTexts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b9843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instancia del modelo\n",
    "model = TfidfVectorizer(use_idf=False,  ## <- \n",
    "                        norm=None,\n",
    "                        ngram_range=(1, 1),\n",
    "                        binary=True     ## <- \n",
    "                        )\n",
    "\n",
    "## Ajuste del modelo y retorno de TF matrix\n",
    "tf_sparse = model.fit_transform(cleanTexts)\n",
    "\n",
    "## Extraer Vocabulario creado por el modelo (dict :: key (word), value (index))\n",
    "vocabulary = model.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "features = [f for f, _ in features]\n",
    "\n",
    "tf_table = DataFrame(tf_sparse.toarray(), columns=features)\n",
    "tf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b021bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find relevant itemsets\n",
    "itemsets = apriori(tf_table.iloc[:, :50].astype(bool), \n",
    "                   min_support=0.05,\n",
    "                   low_memory=True, \n",
    "                   use_colnames=True)\n",
    "itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3987c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find relevant association's rules \n",
    "rules = association_rules(itemsets, metric = 'lift', min_threshold=5)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort rules by confidece and lift \n",
    "sorted_rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display sorted rules\n",
    "sorted_rules[['antecedents', 'consequents', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4cd7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
